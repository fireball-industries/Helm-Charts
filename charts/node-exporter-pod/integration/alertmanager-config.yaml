---
# AlertManager Configuration for Node Exporter Alerts
# Add this to your alertmanager.yml configuration

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@fireball-industries.com'
  smtp_auth_username: 'alertmanager@fireball-industries.com'
  smtp_auth_password: 'changeme'

# Alert routing configuration
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  
  routes:
    # Critical node hardware alerts (immediate notification)
    - match:
        severity: critical
        alertname: ~"Critical.*|.*Critical"
      receiver: 'oncall-pager'
      group_wait: 0s
      repeat_interval: 5m
    
    # Temperature alerts (route to hardware team)
    - match_re:
        alertname: "(High|Critical)Temperature|ThermalThrottling|FanFailure"
      receiver: 'hardware-team'
      group_wait: 30s
      repeat_interval: 1h
    
    # Disk space alerts
    - match_re:
        alertname: "DiskSpace.*|DiskWillFillIn.*"
      receiver: 'storage-team'
      group_wait: 5m
      repeat_interval: 4h
    
    # Memory/CPU pressure
    - match_re:
        alertname: "(High|Critical)(CPU|Memory).*"
      receiver: 'platform-team'
      group_wait: 2m
      repeat_interval: 2h
    
    # OOM killer alerts (high priority)
    - match:
        alertname: OOMKillerActive
      receiver: 'oncall-pager'
      group_wait: 0s
      repeat_interval: 15m
    
    # Warning-level alerts (less urgent)
    - match:
        severity: warning
      receiver: 'slack-warnings'
      group_wait: 5m
      repeat_interval: 24h

# Inhibition rules (suppress alerts based on other alerts)
inhibit_rules:
  # Inhibit warning if critical is firing for same alertname
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
  
  # Inhibit disk warnings if critical disk alert is firing
  - source_match:
      alertname: 'DiskSpaceCritical'
    target_match_re:
      alertname: 'DiskSpace.*'
    equal: ['instance', 'mountpoint']
  
  # Inhibit node alerts if node is not ready
  - source_match:
      alertname: 'NodeNotReady'
    target_match_re:
      alertname: '.*'
    equal: ['instance']

# Receivers configuration
receivers:
  # Default receiver (Slack)
  - name: 'default'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts'
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        send_resolved: true
  
  # On-call pager (PagerDuty)
  - name: 'oncall-pager'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ template "pagerduty.default.description" . }}'
        severity: '{{ if eq .CommonLabels.severity "critical" }}critical{{ else }}error{{ end }}'
        client: 'Prometheus/Fireball Industries'
        client_url: '{{ template "pagerduty.default.clientURL" . }}'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          num_resolved: '{{ .Alerts.Resolved | len }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
  
  # Hardware team (Email + Slack)
  - name: 'hardware-team'
    email_configs:
      - to: 'hardware@fireball-industries.com'
        headers:
          Subject: '[{{ .Status | toUpper }}] Hardware Alert: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'
        send_resolved: true
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#hardware-alerts'
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        send_resolved: true
  
  # Storage team (Email)
  - name: 'storage-team'
    email_configs:
      - to: 'storage@fireball-industries.com'
        headers:
          Subject: '[{{ .Status | toUpper }}] Storage Alert: {{ .GroupLabels.alertname }}'
        send_resolved: true
  
  # Platform team (Slack)
  - name: 'platform-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#platform-alerts'
        send_resolved: true
  
  # Warning-level Slack notifications
  - name: 'slack-warnings'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#monitoring-warnings'
        color: 'warning'
        send_resolved: true
        title: 'Warning: {{ .GroupLabels.alertname }}'

# Custom templates (optional)
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Example custom template file (save as /etc/alertmanager/templates/node-exporter.tmpl):
# {{ define "slack.default.title" }}
# [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
# {{ end }}
#
# {{ define "slack.default.text" }}
# {{ range .Alerts }}
# *Node:* {{ .Labels.instance }}
# *Alert:* {{ .Labels.alertname }}
# *Severity:* {{ .Labels.severity }}
# *Summary:* {{ .Annotations.summary }}
# *Description:* {{ .Annotations.description }}
# {{ if .Annotations.runbook }}*Runbook:* {{ .Annotations.runbook }}{{ end }}
# {{ end }}
# {{ end }}
